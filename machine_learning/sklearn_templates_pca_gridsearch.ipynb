{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template for guiding principle component analysis (PCA) using Python's Scikit Learn (sklearn) library. Sklearn will be imported as skl. The common convention for import I typically see is from sklearn import [library] or from sklearn.library import [library function]. In my experience this can get pretty confusing especially if custom tools are imported in a similiar manner.\n",
    "\n",
    "**Typical imports when not using the convention above**\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](https://github.com/baumanab/udacity_intro_machinelearning_project/blob/master/final_project/intro_machine_learning_final.ipynb) iPython notebook demonstrates use of this workflow\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import sklearn\n",
    "```\n",
    "### 1. Make a feature array with any fields but label or target.\n",
    "- There are a variety approaches to this and will be covered in another template\n",
    "- TODO: create and link template\n",
    "- **Output:** my_features (1D array)\n",
    "\n",
    "\n",
    "### 2. Scale or standardardize features with the sklearn pre-processing module\n",
    "- Example: \n",
    "\n",
    "```python\n",
    "scaled_pca_data = sklearn.preprocessing.MinMaxscaler().fit_transform(my_features)\n",
    "```\n",
    "\n",
    "### 3. Perform dimensionality reduction (dim)often,  by assigning #components or % variability.\n",
    "- Example: \n",
    "```python\n",
    "perc_var = 0.95\n",
    "pca = sklearn.decomposition.PCA(n_components = perc_var)\n",
    "```\n",
    "- It's a good practice to plot %var vs components or track other metrics during this steop\n",
    "- TODO: helper code for this will be added in a another template and placed here\n",
    "\n",
    "### 4. Implement PCA with a pipeline\n",
    "- Pipeline objects can be implemented to create estimator objects with multiple processing steps,for later implementation into other sklearn modules\n",
    "- TODO: Add example where standardize or scaling is implemented\n",
    "- TODO: Add example links (from references in enron project)\n",
    "- Example: \n",
    "\n",
    "```python\n",
    "my_pipe = sklearn.pipeline.Pipeline(steps= [('pca, pca)]), \n",
    "                           ('my_estimator', my_estimator))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch Tuning\n",
    "\n",
    "### 1. Set up a scoring function and cross validator (cv)\n",
    "- sklearn modules can be used to make both\n",
    "\n",
    "### 2. Set up estimator pipeline\n",
    "- Example: This example uses PCA but any or no feature selection could be used (feature selection not necessary)\n",
    "\n",
    "```python\n",
    "estimator = [('reduce_dim',sklearn.decomposition.PCA(),\n",
    "              ('dec-tree', base_estimator)]\n",
    "\n",
    "my_estimator_pipe = sklearn.pipeline.Pipeline(estimator)\n",
    "\n",
    "```\n",
    "\n",
    "### 3. Set up search paramaters dictionary\n",
    "\n",
    "```python\n",
    "my_params = dict(reduce_dim = [perc_var], my_params = #a param list or tuple)\n",
    "```\n",
    "\n",
    "### 4. Set up search\n",
    "\n",
    "```python\n",
    "my_grid_search = sklearn.grid_search.GridSearchCV(my_estimator_pipe_object,\n",
    "                              my_param_grid_dict, my_scoring_function,\n",
    "                              my_cross_validator)\n",
    "```\n",
    "\n",
    "### 5. Pass data into the grid search via the fit method\n",
    "\n",
    "```python\n",
    "my_grid_search.fit(features, labels)\n",
    "```\n",
    "\n",
    "### 6. Select for the best estimator\n",
    "\n",
    "```python\n",
    "my_best_estimator = my_grid_search.best_estimator\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
