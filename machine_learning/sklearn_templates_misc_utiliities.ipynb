{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" this example borrows heavily from the example\n",
    "    shown on the sklearn documentation:\n",
    "\n",
    "    http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "###############################################################\n",
    "### YOUR CODE HERE\n",
    "###############################################################\n",
    "\n",
    "### import the relevant code and make your train/test split\n",
    "### name the output datasets features_train, features_test,\n",
    "### labels_train, and labels_test\n",
    "\n",
    "### set the random_state to 0 and the test_size to 0.4 so\n",
    "### we can exactly check your result\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.4, random_state=0)\n",
    "\n",
    "###############################################################\n",
    "\n",
    "clf = SVC(kernel=\"linear\", C=1.)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "print clf.score(features_test, labels_test)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "def submitAcc():\n",
    "    return clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle split CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Learner, Quadratic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this exercise we'll examine a learner which has high bias, and is incapable of\n",
    "# learning the patterns in the data.\n",
    "# Use the learning curve function from sklearn.learning_curve to plot learning curves\n",
    "# of both training and testing error. Use plt.plot() within the plot_curve function\n",
    "# to create line graphs of the values.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.learning_curve import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import explained_variance_score, make_scorer\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "\n",
    "size = 1000\n",
    "cv = KFold(size,shuffle=True)\n",
    "score = make_scorer(explained_variance_score)\n",
    "\n",
    "X = np.reshape(np.random.normal(scale=2,size=size),(-1,1))\n",
    "y = np.array([[1 - 2*x[0] +x[0]**2] for x in X])\n",
    "\n",
    "def plot_curve():\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X,y)\n",
    "    print reg.score(X,y)\n",
    "    \n",
    "    # TODO: Create the learning curve with the cv and score parameters defined above.\n",
    "    train_sizes, train_scores, test_scores = learning_curve(reg, X, y, cv=cv,scoring=score)\n",
    "    # TODO: Plot the training and testing curves.\n",
    "    plt.figure()\n",
    "    plt.title(\"Awesome Learning Curve Plot\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    # Sizes the window for readability and displays the plot.\n",
    "    plt.ylim(-.1,1.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Noisy Data Complex Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this exercise we'll examine a learner which has high variance, and tries to learn\n",
    "# nonexistant patterns in the data.\n",
    "# Use the learning curve function from sklearn.learning_curve to plot learning curves\n",
    "# of both training and testing error. Use plt.plot() within the plot_curve function\n",
    "# to create line graphs of the values.\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import explained_variance_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "size = 1000\n",
    "cv = KFold(size,shuffle=True)\n",
    "score = make_scorer(explained_variance_score)\n",
    "\n",
    "X = np.round(np.reshape(np.random.normal(scale=5,size=2*size),(-1,2)),2)\n",
    "y = np.array([[np.sin(x[0]+np.sin(x[1]))] for x in X])\n",
    "\n",
    "def plot_curve():\n",
    "    # YOUR CODE HERE\n",
    "    reg = DecisionTreeRegressor()\n",
    "    reg.fit(X,y)\n",
    "    print reg.score(X,y)\n",
    "\n",
    "     # TODO: Create the learning curve with the cv and score parameters defined above.\n",
    "    train_sizes, train_scores, test_scores = learning_curve(reg, X, y, cv=cv,scoring=score)\n",
    "    # TODO: Plot the training and testing curves.\n",
    "    plt.figure()\n",
    "    plt.title(\"Awesome Learning Curve Plot\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    # Show the result, scaling the axis for visibility\n",
    "    plt.ylim(-.1,1.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_curves(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" Calculates the performance of several models with varying sizes of training data.\n",
    "        The learning and testing error rates for each model are then plotted. \"\"\"\n",
    "    \n",
    "    print \"Creating learning curve graphs for max_depths of 1, 3, 6, and 10. . .\"\n",
    "    \n",
    "    # Create the figure window\n",
    "    fig = pl.figure(figsize=(10,8))\n",
    "\n",
    "    # We will vary the training set size so that we have 50 different sizes\n",
    "    sizes = np.rint(np.linspace(1, len(X_train), 50)).astype(int)\n",
    "    train_err = np.zeros(len(sizes))\n",
    "    test_err = np.zeros(len(sizes))\n",
    "\n",
    "    # Create four different models based on max_depth\n",
    "    for k, depth in enumerate([1,3,6,10]):\n",
    "        \n",
    "        for i, s in enumerate(sizes):\n",
    "            \n",
    "            # Setup a decision tree regressor so that it learns a tree with max_depth = depth\n",
    "            regressor = DecisionTreeRegressor(max_depth = depth)\n",
    "            \n",
    "            # Fit the learner to the training data\n",
    "            regressor.fit(X_train[:s], y_train[:s])\n",
    "\n",
    "            # Find the performance on the training set\n",
    "            train_err[i] = performance_metric(y_train[:s], regressor.predict(X_train[:s]))\n",
    "            \n",
    "            # Find the performance on the testing set\n",
    "            test_err[i] = performance_metric(y_test, regressor.predict(X_test))\n",
    "\n",
    "        # Subplot the learning curve graph\n",
    "        ax = fig.add_subplot(2, 2, k+1)\n",
    "        ax.plot(sizes, test_err, lw = 2, label = 'Testing Error')\n",
    "        ax.plot(sizes, train_err, lw = 2, label = 'Training Error')\n",
    "        ax.legend()\n",
    "        ax.set_title('max_depth = %s'%(depth))\n",
    "        ax.set_xlabel('Number of Data Points in Training Set')\n",
    "        ax.set_ylabel('Total Error')\n",
    "        ax.set_xlim([0, len(X_train)])\n",
    "    \n",
    "    # Visual aesthetics\n",
    "    fig.suptitle('Decision Tree Regressor Learning Performances', fontsize=18, y=1.03)\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_complexity(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" Calculates the performance of the model as model complexity increases.\n",
    "        The learning and testing errors rates are then plotted. \"\"\"\n",
    "    \n",
    "    print \"Creating a model complexity graph. . . \"\n",
    "\n",
    "    # We will vary the max_depth of a decision tree model from 1 to 14\n",
    "    max_depth = np.arange(1, 14)\n",
    "    train_err = np.zeros(len(max_depth))\n",
    "    test_err = np.zeros(len(max_depth))\n",
    "\n",
    "    for i, d in enumerate(max_depth):\n",
    "        # Setup a Decision Tree Regressor so that it learns a tree with depth d\n",
    "        regressor = DecisionTreeRegressor(max_depth = d)\n",
    "\n",
    "        # Fit the learner to the training data\n",
    "        regressor.fit(X_train, y_train)\n",
    "\n",
    "        # Find the performance on the training set\n",
    "        train_err[i] = performance_metric(y_train, regressor.predict(X_train))\n",
    "\n",
    "        # Find the performance on the testing set\n",
    "        test_err[i] = performance_metric(y_test, regressor.predict(X_test))\n",
    "\n",
    "    # Plot the model complexity graph\n",
    "    pl.figure(figsize=(7, 5))\n",
    "    pl.title('Decision Tree Regressor Complexity Performance')\n",
    "    pl.plot(max_depth, test_err, lw=2, label = 'Testing Error')\n",
    "    pl.plot(max_depth, train_err, lw=2, label = 'Training Error')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Maximum Depth')\n",
    "    pl.ylabel('Total Error')\n",
    "    pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
